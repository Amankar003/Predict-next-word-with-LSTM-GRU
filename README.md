# ğŸ§  Predict Next Word with LSTM & GRU

**ğŸ” Natural Language Processing (NLP) | Deep Learning | Next-Word Prediction**

This repository contains a **deep learning-based solution** that predicts the *next word* in a sentence using two powerful sequence models:

âœ” **LSTM (Long Short-Term Memory)**  
âœ” **GRU (Gated Recurrent Units)**

This model learns language patterns and predicts the next word based on text input â€” a foundational technique in language modeling used in keyboards, chatbots, writing assistants, and smart text systems.

---

## ğŸ“ Repository Structure

ğŸ“¦ Predict-next-word-with-LSTM-GRU
â”£ ğŸ“œ Hamlet.txt # Text dataset (Shakespeare corpus)
â”£ ğŸ“œ Predict next word using LSTM & GRU.ipynb # Complete Jupyter notebook
â”£ ğŸ“œ next_word_lstm.h5 # Trained LSTM model
â”£ ğŸ“œ tokenizer.pickle # Tokenizer for text processing
â”— ğŸ“œ README.md # This file


---

## ğŸ§  Project Overview

Next-word prediction is a *sequence modeling task* where the model is trained to predict the most likely next word given a sequence of previous words. This task is core to many NLP applications such as:

âœ” Autocomplete keyboards  
âœ” Text suggestion systems  
âœ” Conversational AI / Chatbots  
âœ” Language modeling research

ğŸ”¥ This project uses a dataset (e.g., *Hamlet.txt*) as training text, then builds both LSTM and GRU-based neural networks to learn patterns and generate predictions. :contentReference[oaicite:0]{index=0}

---

## ğŸ“Œ Key Concepts

### ğŸ§  LSTM (Long Short-Term Memory)

- A type of Recurrent Neural Network (RNN)  
- Designed to **remember long-range dependencies** in sequences  
- Solves vanishing gradient problems seen in vanilla RNNs

### ğŸ” GRU (Gated Recurrent Unit)

- A simplified variant of LSTM  
- Fewer parameters â†’ faster training with comparable performance  
- Useful for many NLP sequence tasks  

Both models are excellent for sequence text tasks like next-word prediction. :contentReference[oaicite:1]{index=1}

---

## ğŸ› ï¸ How It Works (High-Level)

### 1. ğŸ“š Data Preparation

- Load raw text corpus (`Hamlet.txt`)
- Clean and normalize the text
- Tokenize into words
- Create input sequences of words
- Pad sequences so each input is the same length

### 2. ğŸ§® Model Training

- Use Tokenizer to convert words â†’ integer indices
- Train LSTM and/or GRU model to learn context patterns
- Optimize model with backpropagation

### 3. ğŸ§ª Prediction

Given a seed sequence (e.g., `"to be or"`), the model predicts the next word by:

- Converting the seed text to tokens
- Feeding into the model
- Outputting the most probable next word

ğŸ“Œ After training, the model is **exported** (`next_word_lstm.h5`) and can be loaded anytime for predictions.

---

## ğŸ§¾ Notebook: Step-by-Step

Open the main notebook:

```bash
Predict next word using LSTM & GRU.ipynb

The notebook contains:

ğŸ“Œ Loading dataset
ğŸ“Œ Text preprocessing
ğŸ“Œ Building the model
ğŸ“Œ Training loops
ğŸ“Œ Evaluation & sample predictions


| Tool               | Purpose                          |
| ------------------ | -------------------------------- |
| Python             | Core programming language        |
| TensorFlow / Keras | Neural networks                  |
| Jupyter Notebook   | Interactive coding + experiments |
| NLP Tokenizer      | Text preprocessing               |
